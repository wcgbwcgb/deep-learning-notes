{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3ab3cb",
   "metadata": {},
   "source": [
    "# Reading Notes: \"Attention Is All You Need\"\n",
    "Author: Muyang Han\n",
    "Date: 2025-12-21\n",
    "Status: Step 1 completed (rough read, grasped motivation and core problem)\n",
    "Date: 2025-12-21\n",
    "Status: Step 2 completed (detailed read, understood architecture and mechanisms)\n",
    "\n",
    "## Overview\n",
    "This notebook records my notes and understanding of the Transformer paper. \n",
    "\n",
    "**Progress:**\n",
    "- âœ… Step 1: Skimmed the paper, understood motivation and core problem\n",
    "- âœ… Step 2: Completed detailed reading, analyzed architecture components\n",
    "- ðŸ”„ Step 3: Code implementation (planned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484232f9",
   "metadata": {},
   "source": [
    "## Why Transformer\n",
    "Before the Transformer model, sequential modeling mainly relied on RNNs and CNNs. However, these models have obvious downsides. In particular, the RNN model suffers from the limitation of its inherently sequential computation that hinders prallelization. The Transformer model, on the other hand, abandons recurrence, relying only on the attention mechanism, enabling significantly more parallelization, more efficient training and better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb52c16",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "### Encoder Block\n",
    "1. Divide the input into tokens, and map them into vectors by using embedding layer.\n",
    "2. Add positional encoding to the vector to get a input matrix X.\n",
    "3. Multiply X matrix with W_Q, W_K, W_V to get Q,K,V matrix. Here, W_Q, W_K, W_V are not single matrix. It is multiple independent W_Qs, W_Ks, W_Vs. Therefore, we will get multiple Q, K and V.\n",
    "4. Each head use the Scaled Dot-Product Attention to get a matrix containing context-aware representation. The output of all heads are concatenated and pass through a linear layer.\n",
    "5. Use residual connection and layer norm to enhance stability.\n",
    "6. Put the matrix into a Feed Forward Network.\n",
    "7. Apply a second Add & Norm\n",
    "\n",
    "### Decoder Block\n",
    "1. Same steps as the Encoder Block.\n",
    "2. Instead of Multi-Head Attention, the decoder block uses a masked Multi-Head Attention.\n",
    "3. After Add & Norm, the Q is send to the second Multi-Head Attention Block. K and V are from the encoder.\n",
    "4. Feed Forward Network and Add & Norm.\n",
    "\n",
    "Finally, the output of decoder will be send to a linear layer, and then convert into probabilities by using softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads == embed_size), \"embed size need to be divisible by heads\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
