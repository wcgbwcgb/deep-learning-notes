{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3ab3cb",
   "metadata": {},
   "source": [
    "# Reading Notes: \"Attention Is All You Need\"\n",
    "Author: Muyang Han\n",
    "Date: 2025-12-21\n",
    "Status: Step 1 completed (rough read, grasped motivation and core problem)\n",
    "\n",
    "## Overview\n",
    "This notebook records my notes and understanding of the Transformer paper. \n",
    "Currently, Step 1 has been completed:\n",
    "- Skimmed the paper\n",
    "- Understood the motivation behind Transformer\n",
    "- Identified limitations of previous sequential models (RNN/CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484232f9",
   "metadata": {},
   "source": [
    "## Why Transformer\n",
    "Before the Transformer model, sequential modeling mainly relied on RNNs and CNNs. However, these models have obvious downsides. In particular, the RNN model suffers from the limitation of its inherently sequential computation that hinders prallelization. The Transformer model, on the other hand, abandons recurrence, relying only on the attention mechanism, enabling significantly more parallelization, more efficient training and better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb52c16",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
