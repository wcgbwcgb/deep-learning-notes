{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebed7490",
   "metadata": {},
   "source": [
    "## Some Logistic Regression Concepts\n",
    "$z = w^{t}x + b$  \n",
    "$\\hat{y} = a = \\sigma(z)$  \n",
    "$L(a, y) = -(y\\log{a}+(1-y)\\log{(1-a)})$  \n",
    "\n",
    "process of foreward and backward propagation:\n",
    "<img src=\"image/propagation.png\" style=\"width:80%;\">\n",
    "\n",
    "we get the cost function by all the calculation at the front. We can modify w and b by using backward propagation by calculating the derivatives using chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fc08d",
   "metadata": {},
   "source": [
    "## Some Calculations\n",
    "<img src=\"image/calculations.jpg\" style=\"width:50%;\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f9eea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500022.044102834\n",
      "numpy: 3.172636032104492ms\n",
      "0.5401602835576047\n",
      "numpy: 1403.7532806396484ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a = np.random.rand(10000000)\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "t1 = time.time()\n",
    "c = np.dot(a,b)\n",
    "t2 = time.time()\n",
    "print(c)\n",
    "print(\"numpy: \"+ str(1000*(t2-t1)) + \"ms\")\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(10000000):\n",
    "    c = a[i] * b[i]\n",
    "t2 = time.time()\n",
    "print(c)\n",
    "print(\"numpy: \"+ str(1000*(t2-t1)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4dae6",
   "metadata": {},
   "source": [
    "**By the experiment above, I found out that using numpy to do matrix calculations is much more efficient.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90444755",
   "metadata": {},
   "source": [
    "## Vectorizing Logistic Regression\n",
    "x is the input matrix, having a shape of ($n_x$, m), which means every column is a training example, and each row represents each feature.\n",
    "\n",
    "$w^T$ is a row vectors. By multipling $w^T$ and x, and then plus b, we can get z\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdfea8",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "[i] represents data in i-th layer  \n",
    "\n",
    "neural network is just repeating z and a calculations  \n",
    "\n",
    "There are three layers in a neural network: input layer, hidden layer, output layer\n",
    "- input layer: x or $a^{[0]}$\n",
    "- hidden layer: something like $a^{[1]}$  \n",
    "$a^{[1]}_1$ means hidden layer 1 node 1\n",
    "\n",
    "When we count layer, we don't count input layer. When a neural network has 1 input layer, 1 hidden layer and 1 output layer, it is a two-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ce277",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "sigmoid function is an example of activation function.  \n",
    "\n",
    "tanh(z) is also an example of activation function, and it works better than sigmoid function, since it can generates an average of 0 instead of 0.5. However, it is not useful for binary classification.  \n",
    "\n",
    "Therefore, we can use tanh(z) in hidden layer, sigmoid function for the output layer.\n",
    "\n",
    "Both tanh(z) and sigmoid function's slope at the end is close to zero, which will slow down gradient descent. Therefore, we can use ReLU function. \n",
    "\n",
    "We don't use linear function in hidden layer, because it can not learn any non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab827c3",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "- learning rate $\\alpha$\n",
    "- number of iterations\n",
    "- number of hidden layer L\n",
    "- number of hidden units\n",
    "- choice of activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a161ef1",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "<img src=\"image/process.png\" style=\"width:50%;\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83ce0a",
   "metadata": {},
   "source": [
    "## Implementing Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "w[l]: (n[l], n[l-1])\n",
    "b[l]: (n[l], 1)\n",
    "'''\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "\n",
    "    '''\n",
    "    num_layer: number of hidden layer\n",
    "    layer_size: units of hidden layer\n",
    "\n",
    "    weights and bias are hidden layers\n",
    "    '''\n",
    "    def __init__(self, layer_size: list):\n",
    "        self.num_layer = len(layer_size)\n",
    "        self.layer_size = layer_size\n",
    "\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "\n",
    "        # initializing weights and bias\n",
    "        for i in range(len(layer_size)-1):\n",
    "            self.weights.append(np.random.rand(layer_size[i+1], layer_size[i]) * 0.01)\n",
    "            self.bias.append(np.zeros((layer_size[i+1], 1)))\n",
    "\n",
    "    \n",
    "    # activation function\n",
    "    def ReLU():\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78be93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.00079833, 0.00033678, 0.00597505, 0.0065092 ],\n",
      "       [0.00112352, 0.00494675, 0.0041837 , 0.00972023],\n",
      "       [0.00460516, 0.00712167, 0.00108812, 0.00651455],\n",
      "       [0.00930487, 0.00247874, 0.00938701, 0.00063321]]), array([[0.00954513, 0.00402281, 0.00719258, 0.00356439]])]\n"
     ]
    }
   ],
   "source": [
    "# the first element in [4,4,1] is a[0]\n",
    "model = MyNeuralNetwork([4,4,1])\n",
    "print(model.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cd3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
