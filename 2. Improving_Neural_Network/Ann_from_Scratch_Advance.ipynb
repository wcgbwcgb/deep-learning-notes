{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43790e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee00c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "ANN code from the last chapter\n",
    "adding ________\n",
    "'''\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "\n",
    "    '''\n",
    "    num_layer: number of layer including input layer\n",
    "    layer_size: units of hidden layer\n",
    "\n",
    "    weights and bias are hidden layers\n",
    "    '''\n",
    "    def __init__(self, layer_size: list, L2: bool = False, output_activation: str = \"sigmoid\", lambd = \"0.1\"):\n",
    "        self.num_layer = len(layer_size)\n",
    "        self.layer_size = layer_size\n",
    "        self.output_activation = output_activation\n",
    "        self.L2 = L2\n",
    "        self.lambd = lambd\n",
    "\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "\n",
    "        # initializing weights and bias\n",
    "        for i in range(len(layer_size)-1):\n",
    "            self.weights.append(np.random.rand(layer_size[i+1], layer_size[i]) * 0.01)\n",
    "            self.bias.append(np.zeros((layer_size[i+1], 1)))\n",
    "\n",
    "\n",
    "    # forward propagation\n",
    "    def forward(self, a0):\n",
    "        a = a0\n",
    "        self.z_value = []\n",
    "        self.a_value = [a0]\n",
    "        \n",
    "        for idx, (w,b) in enumerate(zip(self.weights, self.bias)):\n",
    "            z = np.dot(w, a) + b\n",
    "            if idx == len(self.weights) - 1:  # use sigmoid for output layer\n",
    "                if self.output_activation == \"sigmoid\":\n",
    "                    a = self.sigmoid(z)\n",
    "                elif self.output_activation == \"softmax\":\n",
    "                    a = self.softmax(z)\n",
    "            else:  # hidden layers use ReLU function\n",
    "                a = self.ReLU(z)\n",
    "            self.z_value.append(z)\n",
    "            self.a_value.append(a)\n",
    "            \n",
    "        return a\n",
    "    \n",
    "\n",
    "    # backward propagation\n",
    "    def backward(self, y):\n",
    "        m = y.shape[1]\n",
    "        y_hat = self.a_value[-1]\n",
    "        L = self.num_layer - 1\n",
    "        dw_value = [0] * L\n",
    "        db_value = [0] * L\n",
    "\n",
    "        if self.output_activation == \"sigmoid\":\n",
    "            dz = self.a_value[len(self.a_value)-1] - y  # dL/dz\n",
    "        elif self.output_activation == \"softmax\":\n",
    "            dz = y_hat - y\n",
    "        a_prev = self.a_value[-2]\n",
    "        dw_value[L-1] = (1/m) * np.dot(dz, a_prev.T)  # the last index of dw_value\n",
    "        db_value[L-1] = (1/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in reversed(range(L-1)): # start from L-2\n",
    "            a_prev = self.a_value[i] # get a from previous layer. a has a[0] initially, so we can use i directly\n",
    "            da = np.dot(self.weights[i+1].T, dz)\n",
    "            dz = self.ReLU_backward(da, self.z_value[i])\n",
    "            dw_value[i] = (1/m) * np.dot(dz, a_prev.T)\n",
    "            db_value[i] = (1/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "\n",
    "        return dw_value, db_value\n",
    "    \n",
    "    # update w and b\n",
    "    def update_params(self, dw_value, db_value, alpha=0.01):\n",
    "        for i in range(len(dw_value)):\n",
    "            self.weights[i] -= alpha*dw_value[i]\n",
    "            self.bias[i] -= alpha*db_value[i]\n",
    "    \n",
    "\n",
    "    # activation derivatives\n",
    "    def sigmoid_backward(self, da, z):\n",
    "        return da * self.sigmoid(z) * (1-self.sigmoid(z))\n",
    "    \n",
    "    def ReLU_backward(self, da, z):\n",
    "        dz = np.array(da, copy=True)\n",
    "        dz[z <= 0] = 0\n",
    "        return dz\n",
    "    \n",
    "    # cost function\n",
    "    def compute_cost(self, y_hat, y):\n",
    "        eps = 1e-8\n",
    "        m = y.shape[1]\n",
    "        if self.output_activation == \"sigmoid\":\n",
    "            cost = -(1/m) * np.sum(y*np.log(y_hat + eps) + (1-y)*np.log(1-y_hat + eps))\n",
    "        elif self.output_activation == \"softmax\":\n",
    "            cost = -(1/m) * np.sum(y*np.log(y_hat + eps))\n",
    "\n",
    "        ## L2 regularization\n",
    "        if self.L2:\n",
    "            norm = 0\n",
    "            for w in self.weights:\n",
    "                norm += np.sum(w**2)\n",
    "            cost += self.lambd/(2*m)*norm\n",
    "        return cost\n",
    "\n",
    "    # activation functions\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        t = np.exp(z)\n",
    "        a = t/np.sum(t)\n",
    "        return a\n",
    "        \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
