{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699c99ba",
   "metadata": {},
   "source": [
    "## Hyperparamteres\n",
    "\n",
    "most important:\n",
    "- $\\alpha$\n",
    "\n",
    "second important:\n",
    "- $\\beta$ (momentum term)\n",
    "- mini-batch size\n",
    "- number of hidden units\n",
    "\n",
    "third important:\n",
    "- number of layers\n",
    "- learning rate decay\n",
    "\n",
    "rarely changes:\n",
    "- $\\beta_1$ = 0.9, $\\beta_2$ = 0.999，$\\epsilon$ = $10^{-8}$\n",
    "\n",
    "Instead of grid serach, you can randomly choose a set of value, and see which set of value works the best.\n",
    "\n",
    "Coarse to fine: search for hyperparameter in a large square first, and then pinpoint the smaller square that works the best to find the optimized hyperparameters.\n",
    "\n",
    "#### Using an appropriate scale\n",
    "$\\alpha = 0.0001$ to 1\n",
    "\n",
    "When searching for alpha, do not choose randomly on the linear scale. Instead, use a logistic scale.\n",
    "\n",
    "r = -4 * np.random.rand()  <-- r $\\in$ [-4, 0]  \n",
    "$\\alpha$ = $10^r$\n",
    "\n",
    "\n",
    "#### approches\n",
    "panda approach: babysitting one model, change hyperparameter everyday\n",
    "\n",
    "cavier approach: training many models in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d9940",
   "metadata": {},
   "source": [
    "## Normalization Activations in a network\n",
    "We normalize z to speed up training\n",
    "\n",
    "1. $\\mu = \\frac{1}{m}\\sum_i z^{(i)}$\n",
    "2. $\\sigma^2 = \\frac{1}{m}\\sum_i (z^{(i)}-\\mu)^2$\n",
    "3. $z^{(i)}_norm = \\frac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2+\\varepsilon}}$  --> making every activation into a distribution with mean of 0 and std with 1\n",
    "4. $z̃^{(i)} = γ * z^{(i)}_{norm} + β$  --> We don't want 0 and 1, so we add learnable variable gamma and beta.\n",
    "\n",
    "We can use gradient descent, or other optimization algorithm to update the value of gamma and beta.\n",
    "\n",
    "covariate shift: distribution of x changes, but the function mapping from x to y does not change\n",
    "\n",
    "If the distribution of x changes, you might need to retrain your learning algorithm, even if the ground truth functiond does not change.\n",
    "\n",
    "#### Batch Norm at Test Time\n",
    "At test time, we can not calculate the mu and sigma for out training batch, since we can not divide it into many mini-batches. Therefore, we estimate an $\\mu$ and $\\sigma^2$ by use an exponentially weighted average of the training batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23724fa",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "- C: number of classes \n",
    "\n",
    "If layer L is softman layer:\n",
    "1. t = $e^{(z^{[l]})}$\n",
    "2. a{[l]} = $\\frac{e^{z{[l]}}}{\\sum_{j=1}^4 t_i}$\n",
    "\n",
    "For example\n",
    "1. $z^{[l]} = \\begin{bmatrix}\n",
    "5 \\\\\n",
    "2 \\\\\n",
    "-1 \\\\\n",
    "3 \n",
    "\\end{bmatrix}$ \n",
    "2. $t = \\begin{bmatrix}\n",
    "e^5 \\\\\n",
    "e^2 \\\\\n",
    "e^{-1} \\\\\n",
    "e^3 \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "148.4 \\\\\n",
    "7.4 \\\\\n",
    "0.4 \\\\\n",
    "20.1 \n",
    "\\end{bmatrix} \\sum_{j=1}^4t_j = 176.3$ \n",
    "3. $a^{[l]} = \\frac{t}{176.3}$\n",
    "4. node 1: $\\frac{e^5}{176.3} = 0.842$  \n",
    "node2: 0.042  \n",
    "node3: 0.002  \n",
    "node4: 0.114\n",
    "\n",
    "\n",
    "#### Loss Function\n",
    "$L(\\hat{y}, y) = -\\sum_{j=1}^4 y_jlog(\\hat{y}_j)$\n",
    "example: y = $\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}$, $\\hat{y} = \\begin{bmatrix}\n",
    "0.3 \\\\\n",
    "0.2 \\\\\n",
    "0.1 \\\\\n",
    "0.4 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "if we want the loss to be small, we need $\\hat{y}_j$ as large as possible\n",
    "\n",
    "$J(w^{[1]}, b^{[1]}, ...) = \\frac{1}{m}\\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})$\n",
    "\n",
    "\n",
    "#### Gradient Descent with Softmax\n",
    "$\\frac{\\partial L}{\\partial z_j} = \\hat{y}_j - y_j$\n",
    "\n",
    "change the last layer's derivative into the equation above."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
