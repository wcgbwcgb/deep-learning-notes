{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad4126e",
   "metadata": {},
   "source": [
    "## train/dev/test sets\n",
    "train set: train model  \n",
    "dev set: evaluate model and change hyperparameters  \n",
    "test set: give a unbiased final performance  \n",
    "\n",
    "### ratio\n",
    "- small dataset(like 1000 or 10000): 60%/20%/20%\n",
    "- larger dataset(1,000,000): 98%/1%/1% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45609e0c",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "<img src=\"image/note2/bav.png\" style=\"width:70%;\">\n",
    "<img src=\"image/note2/td.png\" style=\"width:70%;\">\n",
    "\n",
    "High bias: model performance poorly, having a high error percent  \n",
    "High variance: the error percent between training set and dev set has a significant difference  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88b977",
   "metadata": {},
   "source": [
    "## Basic Recipe for Machine Learning\n",
    "\n",
    "1. High bias --> does not perform well on training set --> bigger network(more hidden units or layers), train longer, try new NN architectures, try advanced optimization algorithm \n",
    "\n",
    "2. High variance --> looking at dev set performance --> more data, regularization, or try more appropriate NN architecture\n",
    "\n",
    "3. Low bias and variance --> done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52792df3",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "regularization can prevent overfitting and reduce variance    \n",
    "\n",
    "L2 regularization: \n",
    "$$\n",
    "J(w, b) = \\frac{1}{m}L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\|\\mathbf{w}\\|^2_2\n",
    "$$ \n",
    "where $$\\|\\mathbf{w}\\|^2_2 = w^Tw$$ \n",
    "\n",
    "meaning ***it adds the squares of all individual elements in w.*** It's called Frobenius norm.\n",
    "\n",
    "L1 regularization:\n",
    "\n",
    "changing the regularization term into\n",
    "$$\n",
    "\\frac{\\lambda}{2m}\\|\\mathbf{w}\\|_1\n",
    "$$\n",
    "\n",
    "w will become sparse, meaning w will have a lot of zeros(helps to compress model).\n",
    "\n",
    "$\\lambda$ is called regularization parameter. It is a hyperparameter.\n",
    "\n",
    "\n",
    "When doing back propogation, \n",
    "\n",
    "$dw^{[l]} = (backprop) + \\frac{\\lambda}{m}w^{[l]}$\n",
    "\n",
    "$w^{[l]} := w^{[l]} - \\alpha dw^{[l]}$\n",
    "\n",
    "$w^{[l]} := w^{[l]} - \\alpha (backprop) - \\frac{\\alpha\\lambda}{m}w^{[l]}$\n",
    "\n",
    "L2 regularization is also called weight decay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402fbd6",
   "metadata": {},
   "source": [
    "## How does regularization prevent overfitting?\n",
    "\n",
    "1. zeroing out some impact of hidden units  --> simplifing NN  --> change from high variance to high bias\n",
    "\n",
    "2. w decrease --> z closer to 0 --> every layer becomes closer to linear part of tanh --> solve overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519eb30",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
