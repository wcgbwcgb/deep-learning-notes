{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad4126e",
   "metadata": {},
   "source": [
    "## train/dev/test sets\n",
    "train set: train model  \n",
    "dev set: evaluate model and change hyperparameters  \n",
    "test set: give a unbiased final performance  \n",
    "\n",
    "### ratio\n",
    "- small dataset(like 1000 or 10000): 60%/20%/20%\n",
    "- larger dataset(1,000,000): 98%/1%/1% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45609e0c",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "<img src=\"image/note2/bav.png\" style=\"width:70%;\">\n",
    "<img src=\"image/note2/td.png\" style=\"width:70%;\">\n",
    "\n",
    "High bias: model performance poorly, having a high error percent  \n",
    "High variance: the error percent between training set and dev set has a significant difference  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88b977",
   "metadata": {},
   "source": [
    "## Basic Recipe for Machine Learning\n",
    "\n",
    "1. High bias --> does not perform well on training set --> bigger network(more hidden units or layers), train longer, try new NN architectures, try advanced optimization algorithm \n",
    "\n",
    "2. High variance --> looking at dev set performance --> more data, regularization, or try more appropriate NN architecture\n",
    "\n",
    "3. Low bias and variance --> done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52792df3",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "regularization can prevent overfitting and reduce variance    \n",
    "\n",
    "L2 regularization: \n",
    "$$\n",
    "J(w, b) = \\frac{1}{m}L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\|\\mathbf{w}\\|^2_2\n",
    "$$ \n",
    "where $$\\|\\mathbf{w}\\|^2_2 = w^Tw$$ \n",
    "\n",
    "meaning ***it adds the squares of all individual elements in w.*** It's called Frobenius norm.\n",
    "\n",
    "L1 regularization:\n",
    "\n",
    "changing the regularization term into\n",
    "$$\n",
    "\\frac{\\lambda}{2m}\\|\\mathbf{w}\\|_1\n",
    "$$\n",
    "\n",
    "w will become sparse, meaning w will have a lot of zeros(helps to compress model).\n",
    "\n",
    "$\\lambda$ is called regularization parameter. It is a hyperparameter.\n",
    "\n",
    "\n",
    "When doing back propogation, \n",
    "\n",
    "$dw^{[l]} = (backprop) + \\frac{\\lambda}{m}w^{[l]}$\n",
    "\n",
    "$w^{[l]} := w^{[l]} - \\alpha dw^{[l]}$\n",
    "\n",
    "$w^{[l]} := w^{[l]} - \\alpha (backprop) - \\frac{\\alpha\\lambda}{m}w^{[l]}$\n",
    "\n",
    "L2 regularization is also called weight decay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402fbd6",
   "metadata": {},
   "source": [
    "## How does regularization prevent overfitting?\n",
    "\n",
    "1. zeroing out some impact of hidden units  --> simplifing NN  --> change from high variance to high bias\n",
    "\n",
    "2. w decrease --> z closer to 0 --> every layer becomes closer to linear part of tanh --> solve overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519eb30",
   "metadata": {},
   "source": [
    "## Dropout Regularization\n",
    "\n",
    "Each layer has a probability of eliminating some nodes, so eventually you can get a reduced NN, having a similar effect as regularization.  \n",
    "\n",
    "When implementing dropout, z should time keep_prob to keep the expected value of z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb3522",
   "metadata": {},
   "source": [
    "## Other Regularization Methods\n",
    "1. Data augmentation: includes additional fake trainning examples(like flipping horizontally) to reduce overfitting.  \n",
    "2. Early stopping: stop training halfway. It can prevent overfitting, but it can not get a optimized cost function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c6dd0",
   "metadata": {},
   "source": [
    "## Normalizing Training Sets\n",
    "1. substract out the mean: move the training set until it has a zero mean.\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}  \n",
    "$$\n",
    "$$\n",
    "x := x - \\mu\n",
    "$$\n",
    "\n",
    "2. Nomalize variance: change both variance's height and width into 1\n",
    "$$\n",
    "\\sigma^{2} = \\frac{1}{m} \\sum_{i=1}^m x^{(i)2}\n",
    "$$\n",
    "$$\n",
    "x /= \\sigma\n",
    "$$\n",
    "\n",
    "Without normalizing data, gradient descent might oscillate. If you normalize your data, it is easier and faster to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91038e8b",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding Gradients\n",
    "<img src=\"image/note2/vanishing.png\" style=\"width:70%;\">  \n",
    "\n",
    "If all the node in the graph above has a small w like 0.5, then $\\hat{y}$ will be very small, since each layer will time 0.5 to the z. If w has a value of 1.5, then $\\hat{y}$ will be very large. It also happens in back propagation, causing inefficient update of w at the initial layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a1ea4",
   "metadata": {},
   "source": [
    "## Weight Initialization for Deep Network\n",
    "When initializing w with np.random.randn(), we should multiply by a standard deviation to make sure w is in an appropriate range, preventing gradient exlopsion or vanishing.\n",
    "- ReLU(He initialization): $\\sqrt{\\frac{2}{n^{[l-1]}}}$\n",
    "- tanh(Xavier initialization): $\\sqrt{\\frac{1}{n^{[l-1]}}}$\n",
    "- other initialization: $\\sqrt{\\frac{2}{n^{[l-1]}+n^{[l]}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c67055",
   "metadata": {},
   "source": [
    "## Numerical Approximation of Gradient\n",
    "$\\frac{f(\\theta+\\varepsilon)-f(\\theta-\\varepsilon)}{2\\varepsilon} \\approx f'(x)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8993234",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "1. Take $w^{[1]}$, $b^{[1]}$, ... , $w^{[l]}$, $b^{[l]}$ and reshape into a big vector $\\theta$.\n",
    "\n",
    "2. cost function --> J($\\theta$) = J($\\theta_1$, $\\theta_2$, $\\theta_3$, ...)\n",
    "\n",
    "3. Take $dw^{[1]}$, $db^{[1]}$, ... , $dw^{[l]}$, $db^{[l]}$ and reshape into a big vector $d\\theta$.\n",
    "\n",
    "4. for each i: $d\\theta_{appox}^{[i]} = \\frac{J(\\theta_1, \\theta_2, ..., \\theta_i + \\varepsilon, ...) - J(\\theta_1, \\theta_2, ..., \\theta_i - \\varepsilon, ...)}{2\\varepsilon} \\approx \\frac{\\partial J}{\\partial\\theta_i} = d\\theta[i]$ \n",
    "\n",
    "5. compute Euclidean distance between $d\\theta[i]_{appox}$ and $d\\theta[i]$ --> $\\| \\mathbf{d\\theta}_{approx} - \\mathbf{d\\theta} \\|_2$​\n",
    "\n",
    "6. Check the ratio, the relative error by using the formula $\\frac{\\| \\mathbf{d\\theta}_{approx} - \\mathbf{d\\theta} \\|_2}{\\|{d\\theta}_{approx} \\mathbf\\|_2 + \\| d\\theta\\mathbf\\|_2}$​\n",
    "\n",
    "7. If the ratio is nearly $10^{-7}$, then it is probably correct. If the ratio is nearly $10^{-5}$, then there might be some minor errors. If the ratio is nearly $10^{-3}$, then it is incorrect.\n",
    "\n",
    "Don't use gradient checking in training, only use it in debugging.\n",
    "\n",
    "When using regularization, remember to include the reglarization term's derivative in back propagation\n",
    "\n",
    "Gradient Checking does not work with dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfddeea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
